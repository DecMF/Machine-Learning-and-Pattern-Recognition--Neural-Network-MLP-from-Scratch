# Implementa√ß√£o de MLP From Scratch

Este projeto foi desenvolvido como parte da disciplina **MO444**, ministrada pela professora **Sandra**, e contou com a colabora√ß√£o de **Rafael Simionato(IC/Unicamp)**. O objetivo √© explorar redes neurais e desenvolver um modelo eficiente para a tarefa de classifica√ß√£o, sem o uso direto de pacotes de machine learning como TensorFlow e Scikit-Learn.

## Objetivo do Projeto
Construir uma **rede neural multicamada (MLP - Multi-Layer Perceptron)** para classificar imagens do conjunto de dados **OrganCMNIST**, implementando do zero os principais componentes necess√°rios para o treinamento da rede. O objetivo √© evitar *overfitting* e comparar diferentes arquiteturas e t√©cnicas de otimiza√ß√£o.

## üóÇ Base de Dados
O **OrganCMNIST** √© um dos conjuntos de dados dispon√≠veis no [MedMNIST](https://medmnist.com/). Ele cont√©m imagens 2D extra√≠das de tomografias computadorizadas 3D, focando na classifica√ß√£o de **11 √≥rg√£os diferentes**. As imagens s√£o em escala de cinza e foram padronizadas para **1x28x28 pixels**.

O conjunto de dados √© dividido da seguinte forma:
- **12.975** exemplos de treinamento;
- **2.392** exemplos de valida√ß√£o;
- **8.216** exemplos de teste.

Cada imagem pertence a uma das 11 classes de √≥rg√£os:

| ID | √ìrg√£o | # Imagens |
|:---:|:---|---:|
| 0 | Bexiga | 2.167 |
| 1 | F√™mur Esquerdo | 1.152 |
| 2 | F√™mur Direito | 1.112 |
| 3 | Cora√ß√£o | 1.223 |
| 4 | Rim Esquerdo | 1.947 |
| 5 | Rim Direito | 2.062 |
| 6 | F√≠gado | 5.250 |
| 7 | Pulm√£o Esquerdo | 1.898 |
| 8 | Pulm√£o Direito | 1.931 |
| 9 | P√¢ncreas | 2.102 |
| 10 | Ba√ßo | 2.739 |

A base de dados foi utilizada conforme a divis√£o padr√£o de treino, valida√ß√£o e teste, respeitando a avalia√ß√£o via **balanced accuracy**.

## Implementa√ß√£o da Rede Neural

### **Forward Propagation**
Foi implementada iterando pelas camadas da rede e computando as ativa√ß√µes de cada neur√¥nio com base na seguinte equa√ß√£o:

\[
Z^{(l)} = W^{(l)} A^{(l-1)} + b^{(l)}
\]
\[
A^{(l)} = f(Z^{(l)})
\]

Onde:
- \( W^{(l)} \) s√£o os pesos da camada \( l \);
- \( b^{(l)} \) √© o vi√©s da camada \( l \);
- \( A^{(l-1)} \) s√£o as ativa√ß√µes da camada anterior;
- \( f \) √© a fun√ß√£o de ativa√ß√£o (ReLU nas camadas ocultas e Softmax na camada de sa√≠da).

### **Backward Propagation**
O backrpopagation foi implementada para calcular os gradientes dos pesos e vieses a fim de minimizar a fun√ß√£o de perda **Cross-Entropy**. O gradiente da fun√ß√£o de perda em rela√ß√£o √† sa√≠da da √∫ltima camada foi calculado como:

\[
\frac{\partial J}{\partial Z^{(L)}} = A^{(L)} - Y
\]

Para as camadas intermedi√°rias, utilizamos a regra da cadeia para calcular os gradientes:

\[
\frac{\partial J}{\partial W^{(l)}} = \frac{\partial J}{\partial Z^{(l)}} A^{(l-1)T}
\]
\[
\frac{\partial J}{\partial b^{(l)}} = \sum \frac{\partial J}{\partial Z^{(l)}}
\]
\[
\frac{\partial J}{\partial A^{(l-1)}} = W^{(l)T} \frac{\partial J}{\partial Z^{(l)}}
\]

Dessa forma, iteramos de tr√°s para frente ajustando os pesos de acordo com a regra do gradiente descendente:

\[
W^{(l)} := W^{(l)} - \alpha \frac{\partial J}{\partial W^{(l)}}
\]
\[
b^{(l)} := b^{(l)} - \alpha \frac{\partial J}{\partial b^{(l)}}
\]

Onde \( \alpha \) √© a taxa de aprendizado.

### **Otimizadores Implementados**
Foram desenvolvidos e testados diferentes m√©todos de otimiza√ß√£o para ajustar os pesos da rede:
- **Stochastic Gradient Descent (SGD)**;
- **Adam**.

## Experimentos e Resultados

### **Arquitetura Inicial e Impacto da Regulariza√ß√£o**
Uma rede neural inicial foi projetada com **duas camadas ocultas** contendo **512 e 256 neur√¥nios**, respectivamente. Algumas observa√ß√µes:
- Sem *batch normalization*, houve problemas de instabilidade do gradiente.
- O *early-stopping* interrompeu o treinamento ap√≥s **199 √©pocas**, evitando *overfitting*.
- **Balanced accuracy final:** 66,9% (treino) e 65,1% (valida√ß√£o).

### **Compara√ß√£o de Inicializa√ß√µes: He vs. Xavier**
Testamos as inicializa√ß√µes de pesos **He** e **Glorot/Xavier**:
- **He:** Resultou em converg√™ncia muito lenta, obtendo 55,5% de acur√°cia balanceada.
- **Glorot/Xavier:** Melhor desempenho, alcan√ßando **68,5% (treino) e 67,4% (valida√ß√£o)**.

### **Impacto da Fun√ß√£o de Ativa√ß√£o**
Foi testada a substitui√ß√£o da ativa√ß√£o **ReLU** por **Sigmoid**:
- A ativa√ß√£o **Sigmoid** reduziu significativamente a performance, com acur√°cias pr√≥ximas de **51%**.

## Conclus√µes
Os principais aprendizados deste projeto incluem:
- **A inicializa√ß√£o dos pesos** impacta significativamente a converg√™ncia da rede.
- **ReLU** foi a melhor fun√ß√£o de ativa√ß√£o para camadas ocultas.
- **Glorot/Xavier** teve melhor performance que **He**.
- **SGD** superou **Adam** para essa configura√ß√£o.
- **A profundidade da rede influencia a performance**, mas muitas camadas podem prejudicar o desempenho.

## Pr√≥ximos Passos
- Explorar arquiteturas **mais profundas** com normaliza√ß√£o de batch;
- Testar outras fun√ß√µes de ativa√ß√£o, como **Leaky ReLU**;
- Implementar **dropout** para regulariza√ß√£o;
- Experimentar outros otimizadores como **RMSProp**.



